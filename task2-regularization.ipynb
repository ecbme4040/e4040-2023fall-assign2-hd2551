{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Task 2: Regularizations\n",
    "\n",
    "In this task, you are going to experiment with two popular regularization techniques. \n",
    "\n",
    "**Batch normalization:**\n",
    "\n",
    "When a network becomes deeper, the distribution of the parameters from hidden neurons will also shift greatly. This is one of the reasons that makes it difficult to train a deep neural network.\n",
    "\n",
    "Machine learning teaches us that normalization is a good preprocessing method to deal with such a problem. Therefore, batch normalization deploys a similar idea in neural networks by re-normalizing the hidden values of each layer's outputs before transfering them to the next layer.\n",
    "\n",
    "**Dropout:**\n",
    "\n",
    "In the last assignment, you trained a shallow network and everything looked fine. However, when the network becomes whider and deeper, one of the immediate problems you will be encountering is overfitting. The network overreacts to noise or random errors of the training data while failing to detect the underlying distribution and generalize poorly on the vaidation/test set.\n",
    "\n",
    "Dropout is a well-known method that can mitigate such effects. The core idea behind it is quite simple: rather than updating all trainable parameters each time, it randomly selects a subset of parameters to update and keeps other parameters unaltered.\n",
    "\n",
    "**References:**\n",
    "* https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "* https://arxiv.org/pdf/1502.03167.pdf\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "* https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, val = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_val_raw, y_val = val\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_val = X_val_raw.reshape((X_val_raw.shape[0], X_val_raw.shape[1]**2))\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We've vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Batch Normalization (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a neural network, different network layers may prefer different distribution of the input to function more effectively. However, inputs often come in varying ranges due to the fact that:\n",
    "- Data are often batched in vairous ways, so the statistics of every batch does not always reflect the statistics of the whole input space;\n",
    "- Due to network training, the outputs of preceding layers constantly change even on the same data.\n",
    "\n",
    "Batch normalization propose to address this by inserting an independent layer that transforms the preceding outputs to a **learnable** distribution before sending them as input to the layers after. Specifically, our goals are:\n",
    "1. Obtain a good **estimate** of the mean and variance of the entire input space.\n",
    "2. Use this quantity to **standardize** the input distribution to $0$ mean and unit variance.\n",
    "3. **Learn** an appropriate output mean and variance that the input should be transformed into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particularly, let\n",
    "\n",
    "$$X_b = (x_1, \\dots, x_N)^T \\in R^{N \\times D}, \\quad x_i \\in R^D$$\n",
    "\n",
    "be some batched input data with batch size $N$ where $b$ is the batch index (e.g. $X_1$ denotes the $1$-st batch).\n",
    "\n",
    "Batch normalization translates to two steps:\n",
    "1. Standardization:\n",
    "   $$\n",
    "   \\hat{X}_b = \\frac{X_b - \\hat\\mu}{\\sqrt{\\hat\\sigma^2 + \\epsilon}} = \\begin{bmatrix}\n",
    "   \\frac{x_1 - \\hat\\mu}{\\sqrt{\\hat\\sigma^2 + \\epsilon}}, \\cdots,\n",
    "   \\frac{x_N - \\hat\\mu}{\\sqrt{\\hat\\sigma^2 + \\epsilon}}\n",
    "   \\end{bmatrix}^T \\in R^{N \\times D}\n",
    "   $$\n",
    "   where $\\hat\\mu, \\hat\\sigma^2 \\in R^D$ are the estimated mean and variance of the input space (see below).\n",
    "2. Transformation:\n",
    "   $$\n",
    "   Y_b = \\hat{X}_b \\odot \\gamma + \\beta = \\begin{bmatrix}\n",
    "   \\hat{x}_1 \\odot \\gamma + \\beta, \\cdots,\n",
    "   \\hat{x}_N \\odot \\gamma + \\beta\n",
    "   \\end{bmatrix}^T \\in R^{N \\times D} \\in R^{N \\times D}\n",
    "   $$\n",
    "   where $\\beta, \\gamma \\in R^D$ are **learnable** parameters of desired output mean and vairance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>Note:</strong></span> In the left equalities of the two formulas above, the notations are chosen for simplicity consideration. As is shown by the right equalities, batch normalization is applied to **every sample** in the batch $X_b$, so the computation simply propagates through the batch dimension.\n",
    "\n",
    "<span style=\"color:red\"><strong>Hint:</strong></span> You can also code in this fashion with the help of [broadcasting mechanism](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n",
    "\n",
    "Now, let's take a closer look at what happens in the training and testing stage respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Stage\n",
    "\n",
    "Observing from the above equations, batchnorm is a **parametrized** layer (by $\\hat\\mu, \\hat\\sigma^2, \\beta, \\gamma$). Particularly, $\\beta, \\gamma$ are learned through network optimization (e.g. via gradient descent). One piece of the missing puzzle is the calculation of $\\hat\\mu, \\hat\\sigma^2$.\n",
    "\n",
    "In the world of model training, we often live with the following assumptions:\n",
    "- Training data represents a good and unbiased knowledge of the data space in general.\n",
    "- Testing/validation data *can* be biased and imbalanced.\n",
    "\n",
    "Therefore, in the training stage, it is usually safe to use the **sample mean** and **sample variance** of every batch when standardizing the inputs, i.e.\n",
    "\n",
    "$$\n",
    "\\hat{X}_b = \\frac{X_b - \\bar X_b}{\\sqrt{S^2(X_b) + \\epsilon}} \\quad \\text{where} \\quad\n",
    "\\begin{cases}\n",
    "&\\bar{X}_b = \\frac{1}{N} \\sum_{i=1}^N x_i \\in R^D \\\\\n",
    "&S^2(X_b) = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar{X}_b)^2 \\in R^D\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "And we accumulate the these values to use in the testing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is fairly costly to iterate through the entire dataset to get the quantities when we need them, let alone that they will change everytime when the parameters in the preceding layers are updated. We prefer a way of estimating their values on-the-go, and **moving average** is just the right tool for this.\n",
    "\n",
    "At every batch $b$, we update the moving averages of $\\hat \\mu$ and $\\hat \\sigma^2$ as\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat \\mu \\gets \\beta \\hat \\mu + (1 - \\beta) \\bar{X}_b \\\\\n",
    "\\hat \\sigma^2 \\gets \\beta \\hat \\sigma^2 + (1 - \\beta) S^2(X_b) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "After the training stage is over, our $\\hat \\mu$ and $\\hat \\sigma^2$ will be ready to use in the testing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stage\n",
    "\n",
    "For standardizatoin, since the data now are ***not*** considered unbiased anymore, we cannot use their own sample mean and sample variance. Instead, we apply $\\hat \\mu$ and $\\hat \\sigma^2$ **stored from the training stage**.\n",
    "\n",
    "There is also no reason to further update their moving averages (we don't want the values calculated from the training set to be contaminated by biased test data).\n",
    "\n",
    "And all other calculations simply follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit functions `bn_forward` in **./utils/reg_funcs.py**\n",
    "\n",
    "If the code is running correctly, **mean of a2_bn for train will be very close to 0 and variance of a2_bn will be close to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Checking/verification code. Don't change it.     #\n",
    "####################################################\n",
    "\n",
    "from utils.reg_funcs import bn_forward\n",
    "from utils.reg_funcs import bn_backward\n",
    "\n",
    "np.random.seed(2022)\n",
    "N, D, H1, H2 = 200, 64, 3, 3\n",
    "eps = 1e-5\n",
    "x_in = np.random.randn(N, D)\n",
    "w1 = np.random.randn(D,H1)\n",
    "w2 = np.random.randn(H1,H2)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "\n",
    "# Before batch normalization\n",
    "print(\"mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"var of a2: \", np.var(a2, axis=0))\n",
    "\n",
    "# Test \"train mode\" of forward function\n",
    "# After batch normalization, the mean should be close to zero and var should be close to one. \n",
    "bn_config = {\"epsilon\":eps, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "print(\"(train) mean of a2_bn:\", np.mean(a2_bn, axis=0))\n",
    "print(\"(train) var of a2_bn:\", np.var(a2_bn, axis=0))\n",
    "print('*'*80)\n",
    "print(\"Is mean correct? -\", np.allclose(np.mean(a2_bn, axis=0), np.zeros_like(np.mean(a2_bn, axis=0))))\n",
    "print(\"Is variance correct? -\", np.allclose(np.var(a2_bn, axis=0), np.ones_like(np.var(a2_bn, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Test \"moving average\" and \"test mode\" of the `bn_forward` function. \n",
    "\n",
    "We expect that the test mean & variance of a2 to be fairly close to the real values and the test mean & variance of a2_bn to be close to 0 and 1 respectively.\n",
    "\n",
    "<span style=\"color:red\"><strong>Note:</strong></span> Showing \"**True**\" in this section doesn't guarantee a correct implementation. To verify your code, run the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Checking code. Don't change it.     #\n",
    "#######################################\n",
    "\n",
    "# Test \"moving average\" and \"test mode\" of forward function\n",
    "# Then you are going to run the forward function under \"training mode\" for several times, \n",
    "# and the moving mean and moving var will be close to the real mean and var of the input data.\n",
    "# Next, run the forward function under \"test\" mode and you will see that the mean and var of its \n",
    "# output will be also close to gamma, beta that you have set before.\n",
    "\n",
    "bn_config = {\"epsilon\":1e-8, \"decay\":0.8}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "\n",
    "# collect_data: for calculating real mean and var of a2 later.\n",
    "collect_data = a2\n",
    "np.random.seed(2022)\n",
    "for _ in range(100):\n",
    "    x_in = np.random.randn(N, D)\n",
    "    a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "    collect_data = np.concatenate((collect_data, a2), axis=0)\n",
    "    bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "\n",
    "# compare moving_mean and moving_var with real mean and var.\n",
    "# You should see that they are close to each other.\n",
    "print(\"real mean of data: \", np.mean(collect_data, axis=0))\n",
    "print(\"real var of data: \", np.var(collect_data, axis=0))\n",
    "print(\"moving mean of data: \", bn_config[\"moving_mean\"])\n",
    "print(\"moving var of data: \", bn_config[\"moving_var\"])\n",
    "\n",
    "# \"test mode\" of forward function\n",
    "# After bn_forward, the mean and var of output should be kind of close to gamma and beta.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "print(\"*\"*80)\n",
    "print(\"(test) mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"(test) var of a2: \", np.var(a2, axis=0))\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "print(\"(test) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(test) var of a2_bn: \", np.var(a2_bn, axis=0))\n",
    "print(\"*\"*80)\n",
    "print(\"Is moving mean close? -\", np.allclose(bn_config[\"moving_mean\"], np.mean(collect_data, axis=0), rtol=1e-1))\n",
    "print(\"Is moving variance close? -\", np.allclose(bn_config[\"moving_var\"], np.var(collect_data, axis=0), rtol=1e-1))\n",
    "print(\"Is test mean close? -\", np.allclose(np.mean(a2_bn, axis=0), beta, atol=2e-1))\n",
    "print(\"Is test variance close? -\", np.allclose(np.var(a2_bn, axis=0), gamma, atol=2e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply take the derivatives for back propagation. Assume that the upstream gradient from the final Loss $L$ w.r.t the layer output $Y$ is $\\nabla_Y L = G \\in R^{N \\times D}$, the backward pass is\n",
    "\n",
    "$$\n",
    "\\nabla_{X} L = \\frac{\\gamma}{\\sqrt{\\hat \\sigma^2 + \\epsilon}} \\odot G \\in R^{N \\times D}, \\quad \n",
    "\\nabla_{\\gamma} L = \\mathbb{1}^T (G \\odot \\frac{X - \\hat \\mu}{\\sqrt{\\hat \\sigma^2 + \\epsilon}}) \\in R^D, \\quad\n",
    "\\nabla_{\\beta} L = \\mathbb{1}^T G \\in R^D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Read the code `bn_backward` provided in **\"./utils/reg_funcs.py\"**. Use TensorFlow functions to verify the correctness of the backward function. \n",
    "\n",
    "<span style=\"color:red\"><strong>Hint:</strong></span> Use `tf.GradientTape()`. You may find an example usage in the instructor's verification code from Assignment 1 Task1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After verifying the forward function and save the bn_config.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "da2_bn = np.ones_like(a2)\n",
    "# Test backward function with tensorflow\n",
    "# You will use bn_config = {\"eps\":1e-5, \"decay\":0.9, \"moving_mean\":moving_mean, \"moving_var\":moving_var}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, cache = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "da2, dgamma, dbeta = bn_backward(da2_bn, cache)\n",
    "\n",
    "# results\n",
    "da2_tf = tf.zeros_like(da2)\n",
    "dgamma_tf = tf.zeros_like(dgamma)\n",
    "dbeta_tf = tf.zeros_like(dbeta)\n",
    "\n",
    "###################################################\n",
    "# TODO: verify the backward code. You should use  #\n",
    "# the parameters in bn_config and other variables #\n",
    "# above.                                          #\n",
    "# You should store:                               #\n",
    "#     - da2_tf: gradient of a2_bn w.r.t a2        #\n",
    "#     - dgamma_tf: gradient of a2_bn w.r.t gamma  #\n",
    "#     - dbeta_tf: gradient of a2_bn w.r.t beta    #\n",
    "###################################################\n",
    "# raise NotImplementedError\n",
    "\n",
    "\n",
    "###################################################\n",
    "# ENDTODO #\n",
    "###################################################\n",
    "    \n",
    "# Make comparison\n",
    "print(\"Is a2_bn correct? {}\".format(np.allclose(a2_bn, a2_bn_check)))\n",
    "print(\"Is da2 correct? {}\".format(np.allclose(da2, da2_check)))\n",
    "print(\"Is dgamma correct? {}\".format(np.allclose(dgamma, dgamma_check)))\n",
    "print(\"Is dbeta correct? {}\".format(np.allclose(dbeta, dbeta_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add batch normalization into MLP in `./utils/neuralnets/mlp.py`\n",
    "\n",
    "2. First create a shallow MLP like two-layer network with shape [50(+10)] (a hidden layer with depth 5 and an output layer of 10 classes). Train it without and with batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "3. Then, create a slightly deeper 5-layer MLP network with shape [100,50,50,100(+10)] and train the network with and without batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "4. Make a comparison and describe what you have found in this experiment.\n",
    "\n",
    "<span style=\"color:red\"><strong>Note:</strong></span> This section serves as a proof of concept. You may play arround with the hyperparameters for a bit and build more understanding, but it is not required to achieve some certain accuracy threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on shallow MLP** \n",
    "\n",
    "Here in the demo, we set a large learning rate on purpose. You can play with different learning rates and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neuralnets.mlp import MLP \n",
    "from utils.optimizers import AdamOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a two-layer network without batch normalization.\n",
    "# Here is a demo.\n",
    "use_bn = False\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[50], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_no_bn = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a two-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[50], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_bn = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_shallow_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_shallow_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on deep MLP** \n",
    "\n",
    "Here in the demo, we set a large learning rate on purpose. You can play with different learning rates and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network without batch normalization. Remember to \"use_bn\".\n",
    "use_bn = False\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 50, 50, 100], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_no_bn = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 50, 50, 100], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_bn = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_deep_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_deep_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dropout (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is another straightforward regularization technique that uses randomness to enforce the learning on all hidden neurons.\n",
    "\n",
    "### Training Stage\n",
    "\n",
    "During the forward pass of dropout, each entry in the original data are kept with a certain probability $p$, otherwise they are discarded (set to zero). Assume the input $X \\in R^{N \\times D}$, the forward pass is\n",
    "\n",
    "$$Y = M \\odot X \\in R^{N \\times D}$$\n",
    "\n",
    "where $M \\sim B(N \\times D, p)$ is a boolean mask generated from Binomial distribution.\n",
    "\n",
    "### Testing Stage\n",
    "\n",
    "In the testing stage, randomness is no longer desired, but we still need to retain the mean and variance of the output so that it conforms with the training. Therefore, we use\n",
    "\n",
    "$$Y = p X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dropout_forward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit function `dropout_forward` in `./utils/reg_funcs.py`. If the code is running correctly, you will see that the outputs of the verification code should be close to each other. \n",
    "\n",
    "If the function is correct, then **the output mean should be close to the input mean. Input mean and output test mean should be identical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "from utils.reg_funcs import dropout_forward\n",
    "from utils.reg_funcs import dropout_backward\n",
    "\n",
    "x_in = np.random.randn(500, 500) + 10\n",
    "\n",
    "p = 0.7\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": p}\n",
    "# feedforward\n",
    "out, cache = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"train\")\n",
    "out_test, _ = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"test\") \n",
    "# backward\n",
    "dout = np.ones_like(x_in)\n",
    "dx = dropout_backward(dout, cache)\n",
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "# Check forward correctness\n",
    "print(\"mean_of_input = {}\".format(p*np.mean(x_in)))\n",
    "print(\"mean_of_out = {}\".format(np.mean(out)))\n",
    "print(\"mean_of_out_test = {}\".format(np.mean(out_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add dropout into the MLP in `./utils/neuralnets/mlp.py` and understand how the dropout is added into the MLP.\n",
    "\n",
    "2. Customize your own MLP network. Then, train networks with different $p$ of $\\{0.1, 0.3, 0.5, 0.7, 0.9, 1\\}$. If $p = 1$, then the network is equivalent to the MLP without dropout. \n",
    "\n",
    "3. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "Note that checking/validation code is included below with preselected dropout parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example on how to collect loss and accuracy info\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": 1}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_no_dropout = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\": True, \"keep_prob\": 0.9}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_1 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.7}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_2 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.5}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_3 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.3}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_4 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.1}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_5 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_no_dropout, train_acc_no_dropout, val_acc_no_dropout = hist_no_dropout\n",
    "loss_dropout_1, train_acc_dropout_1, val_acc_dropout_1 = hist_dropout_1\n",
    "loss_dropout_2, train_acc_dropout_2, val_acc_dropout_2 = hist_dropout_2\n",
    "loss_dropout_3, train_acc_dropout_3, val_acc_dropout_3 = hist_dropout_3\n",
    "loss_dropout_4, train_acc_dropout_4, val_acc_dropout_4 = hist_dropout_4\n",
    "loss_dropout_5, train_acc_dropout_5, val_acc_dropout_5 = hist_dropout_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_no_dropout, label=\"no dropout\")\n",
    "plt.plot(loss_dropout_1, label=\"keep_prob=0.9\")\n",
    "plt.plot(loss_dropout_2, label=\"keep_prob=0.7\")\n",
    "plt.plot(loss_dropout_3, label=\"keep_prob=0.5\")\n",
    "plt.plot(loss_dropout_4, label=\"keep_prob=0.3\")\n",
    "plt.plot(loss_dropout_5, label=\"keep_prob=0.1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(train_acc_dropout_1, label=\"keep_prob=0.9\")\n",
    "plt.plot(train_acc_dropout_2, label=\"keep_prob=0.7\")\n",
    "plt.plot(train_acc_dropout_3, label=\"keep_prob=0.5\")\n",
    "plt.plot(train_acc_dropout_4, label=\"keep_prob=0.3\")\n",
    "plt.plot(train_acc_dropout_5, label=\"keep_prob=0.1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(val_acc_dropout_1, label=\"keep_prob=0.9\")\n",
    "plt.plot(val_acc_dropout_2, label=\"keep_prob=0.7\")\n",
    "plt.plot(val_acc_dropout_3, label=\"keep_prob=0.5\")\n",
    "plt.plot(val_acc_dropout_4, label=\"keep_prob=0.3\")\n",
    "plt.plot(val_acc_dropout_5, label=\"keep_prob=0.1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this dropout experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dropout + Batch Normalization (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep network with both dropout and batch normalization.\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": 0.9}\n",
    "use_bn = True\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[128, 64, 32], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_5 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

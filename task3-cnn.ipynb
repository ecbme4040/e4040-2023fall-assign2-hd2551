{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Assignment 2 - Task 3: Convolutional Neural Network (CNN)\n",
    "\n",
    "In this task, you are going to first practice the forward/backward propagation of the convolution operations with NumPy. After that, we will introduce TensorFlow with which you will create your CNN model for an image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "\n",
    "Convolutional neural networks (CNNs) are highly effective for image processing tasks. \n",
    "\n",
    "When one builds a MLP model, each connection is multiplied by its own weight. When the input dimension or the first layer is very large, we need a giant matrix to store the weights. This could easily become a problem in image processing since the dimension of a vectorized image could easily exceed 1000 (consider CIFAR-10 which has images of shape 32Ã—32=1024, yet the resolution is so low). \n",
    "\n",
    "In CNN, the weights are shared: the same filter (also known as 'weights' or 'kernel') moves over the input, and at each position an output value is calculated. This means that the same weights are repetitively applied to the entire input, therefore saving a lot of memory.\n",
    "\n",
    "![Illustration of the CNN](./utils/notebook_images/task3_1.jpg)\n",
    "\n",
    "[Image source](https://developer.apple.com/library/content/documentation/Performance/Conceptual/vImage/ConvolutionOperations/ConvolutionOperations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convolution:__  In the picture above, the input is a 7-by-7 image, and the filter is shown as a blue 3-by-3 grid. The filter overlaps with the top-left corner of the input, and we perform an element-wise multiplication followed by a summation, then put the sum into the output matrix. The filter then moves several pixels right, covering a new input area so a new sum can be derived.\n",
    "\n",
    "__Training:__ One thing to remember is that there would be a lot of filters for each layer in a CNN, and the goal of training is to find the best filters for your task. Each filter tries to capture one specific feature. Typically, in the first convolutional layer which directly looks at your input, the filters try to capture information about color and edges which we know as local features; in higher layers, due to the effect of max-pooling, the receptive-fields of filters becomes large so more global and complex features can be detected. \n",
    "\n",
    "__Architecture:__ For classification tasks, a CNN usually starts with convolution commonly followed by either average-pooling or max-pooling. After that, the feature maps will be flattened so that we could append fully connected layers. Common activation functions include ReLu, ELU in the convolution layers, and softmax in the fully connected layers (to calculate the classification scores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "* __Convolution__: element-wise multiplication followed by summation of your input and one of your filters in the CNN context.\n",
    "* __Filter/kernel/weights__: a grid or a set of grids typically smaller than your input size that moves over the input space to generate output. Each filter captures one type of feature.\n",
    "* __Feature/feature maps__: the output of a hidden layer. Think of it as another representation of your data. \n",
    "* __Pooling__: a downsampling operation that joins local information together, so the higher layers' receptive fields can be bigger. The most seen pooling operation is max-pooling, which outputs the maximum of all values inside the pool.\n",
    "* __Flatten__: a junction between convolution layers and fully connected layers. Used to turn 2-D feature maps into 1-D. For tasks such as image segmentation where the output also needs to be 2-D, this won't be used.\n",
    "* __Border mode__: usually refers to 'VALID' or 'SAME'. Under 'VALID' mode, only when the filter and the input fully overlap can a convolution be conducted; under 'SAME' mode, the output size is the same as the input size (only when the stride is 1), and when the filter and the input don't fully overlap (happens at the edge/corner of input) we pad zeroes (or other designated numbers) and then do convolution.\n",
    "\n",
    "[This site](https://cs231n.github.io/convolutional-networks/) is also a good reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Convolution (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conv2d feedforward\n",
    "\n",
    "Implement a NumPy naive 2-D convolution feedforward function. We ask you to simply do the element-wise multiplication and summation. Do not worry about the efficiency of your functions. Use as many loops as you like.\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Complete the function __conv2d_forward__ in __utils/layer_funcs.py__. After that, run the following cell blocks in Jupyter notebook, which will give the output of your convolution function. Detailed instructions have been given in the comments of __layer_func.py__. __The instructors will look at the output to give credits for this task__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your feedforward result (size :(2, 3, 3, 4)) is: \n",
      "[[[[-0.49018405 -0.35006199 -0.20993993 -0.06981787]\n",
      "   [-0.38629836 -0.25815678 -0.1300152  -0.00187362]\n",
      "   [-0.17199324 -0.03014875  0.11169575  0.25354024]]\n",
      "\n",
      "  [[ 0.07961979  0.21551232  0.35140485  0.48729737]\n",
      "   [ 0.47663201  0.5990752   0.7215184   0.8439616 ]\n",
      "   [ 0.40188702  0.5403632   0.67883938  0.81731556]]\n",
      "\n",
      "  [[ 0.17364167  0.32582076  0.47799985  0.63017894]\n",
      "   [ 0.45442123  0.60064836  0.74687549  0.89310261]\n",
      "   [ 0.28514051  0.43904203  0.59294355  0.74684508]]]\n",
      "\n",
      "\n",
      " [[[-0.29100188 -0.13399998  0.02300192  0.18000383]\n",
      "   [-0.23944372 -0.08598237  0.06747897  0.22094032]\n",
      "   [-0.17536921 -0.01664487  0.14207947  0.3008038 ]]\n",
      "\n",
      "  [[-0.07736276  0.08384953  0.24506183  0.40627412]\n",
      "   [ 0.01328028  0.17370312  0.33412597  0.49454882]\n",
      "   [-0.05893273  0.10486322  0.26865916  0.43245511]]\n",
      "\n",
      "  [[-0.23485056 -0.06579163  0.10326731  0.27232624]\n",
      "   [-0.31023572 -0.13868882  0.03285807  0.20440496]\n",
      "   [-0.32590986 -0.15512849  0.01565288  0.18643424]]]]\n"
     ]
    }
   ],
   "source": [
    "# tf 2.4.0 implementation\n",
    "from utils.layer_funcs import conv2d_forward\n",
    "\n",
    "# Set test parameters.\n",
    "x_shape = (2, 7, 7, 3) #(batch, height, width, channels)\n",
    "w_shape = (3, 3, 3, 4) #(filter_height, filter_width, channels, num_of_filters)\n",
    "channels = w_shape[-1]\n",
    "\n",
    "#Superficial change\n",
    "x = np.linspace(-0.5, 0.1, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.3, 0.2, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.2, 0.3, num=channels)\n",
    "\n",
    "pad = 1\n",
    "stride = 3\n",
    "your_feedforward = conv2d_forward(x, w, b, pad, stride)\n",
    "\n",
    "print(\"Your feedforward result (size :{}) is: \".format(your_feedforward.shape))\n",
    "print(your_feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is your feedforward correct? True\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Verification/checking code. Do not modify it       #\n",
    "######################################################\n",
    "\n",
    "X_tf = tf.constant(x, shape=x_shape)\n",
    "w_tf = tf.constant(w, shape=w_shape)\n",
    "b_tf = tf.constant(b, shape=channels)\n",
    "\n",
    "def conv2d_forward_tf(x, w, b, stride):\n",
    "    # stride in tf.nn.conv2d is in the format: [1, x_movement, y_movement, 1]\n",
    "    feedforward = tf.nn.conv2d(x, w, [1, stride, stride, 1], padding=\"SAME\")\n",
    "    # add bias to the conv network\n",
    "    feedforward = tf.nn.bias_add(feedforward, b)\n",
    "    return feedforward\n",
    "\n",
    "print(\"Is your feedforward correct? {}\".format(np.allclose(\n",
    "    your_feedforward, conv2d_forward_tf(X_tf, w_tf, b_tf, stride)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation (Demo) for 2D Convolution\n",
    "\n",
    "**This function is a demo for NumPy naive 2-D convolution backpropagation.**\n",
    "\n",
    "Implementations could be found in the function __conv2d_backward__ in __utils/layer_funcs.py__. Run the following cell blocks, which will give the output of your backpropagation. No need to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your weights' gradients result (size :(3, 3, 3, 4)) is: \n",
      "[[[[-0.9448206  -1.0686526  -0.3135852  -0.03957435]\n",
      "   [-0.93686473 -1.0602988  -0.30789375 -0.04026189]\n",
      "   [-0.9289089  -1.0519449  -0.3022023  -0.04094943]]\n",
      "\n",
      "  [[-1.3040056  -0.8374328  -0.21998174  0.6414077 ]\n",
      "   [-1.2951415  -0.8254823  -0.21534838  0.63206196]\n",
      "   [-1.2862774  -0.8135318  -0.21071501  0.62271625]]\n",
      "\n",
      "  [[-1.2994598  -0.3068896  -0.25185415  1.1314708 ]\n",
      "   [-1.287848   -0.3005608  -0.25122565  1.1195849 ]\n",
      "   [-1.2762363  -0.294232   -0.25059715  1.107699  ]]]\n",
      "\n",
      "\n",
      " [[[-0.99625653 -0.9673741   0.9784877   0.02301925]\n",
      "   [-0.987849   -0.96097547  0.98191816  0.02328004]\n",
      "   [-0.97944146 -0.95457685  0.98534864  0.02354082]]\n",
      "\n",
      "  [[-0.79276395 -1.3021034   1.0766063   1.2217364 ]\n",
      "   [-0.7856655  -1.2886565   1.0777571   1.2108353 ]\n",
      "   [-0.778567   -1.2752095   1.0789078   1.1999341 ]]\n",
      "\n",
      "  [[-0.10447567 -1.2830693   1.0130789   1.6749119 ]\n",
      "   [-0.09791528 -1.2734449   1.006716    1.6608189 ]\n",
      "   [-0.09135488 -1.2638205   1.0003531   1.646726  ]]]\n",
      "\n",
      "\n",
      " [[[-1.1468793  -1.2304289   1.0914675  -0.3861888 ]\n",
      "   [-1.1350057  -1.222894    1.0924698  -0.37766218]\n",
      "   [-1.1231321  -1.2153591   1.093472   -0.3691356 ]]\n",
      "\n",
      "  [[-1.1311175  -1.6217463   1.2828496   0.45399398]\n",
      "   [-1.1164029  -1.6074849   1.279071    0.45700017]\n",
      "   [-1.1016883  -1.5932235   1.2752925   0.46000633]]\n",
      "\n",
      "  [[-0.32878494 -1.5424399   0.97003525  0.7883872 ]\n",
      "   [-0.32069    -1.5330256   0.96160156  0.786144  ]\n",
      "   [-0.3125951  -1.5236114   0.95316786  0.78390086]]]]\n",
      "Your biases' gradients result is: \n",
      "[ 3.46642345  6.56658685  0.5619718  -5.32340436]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Demo code. Don't change it.                      #\n",
    "####################################################\n",
    "from utils.layer_funcs import conv2d_backward\n",
    "# Set test parameters. Please don't change it.\n",
    "np.random.seed(123)\n",
    "d_top = np.random.normal(size=your_feedforward.shape)\n",
    "your_dw, your_db, d_w_shape = conv2d_backward(d_top, x, w, b, pad, stride)\n",
    "\n",
    "print(\"Your weights' gradients result (size :{}) is: \".format(d_w_shape))\n",
    "print(your_dw)\n",
    "print(\"Your biases' gradients result is: \")\n",
    "print(your_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are your weights' gradients correct? True\n",
      "Are your biases' gradients correct? True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Verification/checking code. Don't change it.     #\n",
    "####################################################\n",
    "d_top_tf = tf.constant(d_top, shape=your_feedforward.shape)\n",
    "def conv2d_backward_tf(x, w, b, d, stride):\n",
    "    # stride in tf implementation is in the format: [1, x_movement, y_movement, 1]\n",
    "\n",
    "    dw_tf =  tf.compat.v1.nn.conv2d_backprop_filter(x, w, d, [1, stride, stride, 1], padding = \"SAME\")\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(b)\n",
    "        y = conv2d_forward_tf(X_tf, w_tf, b, stride) * d\n",
    "    dy_dx = g.gradient(y, b)\n",
    "    return dw_tf, dy_dx\n",
    "\n",
    "print(\"Are your weights' gradients correct? {}\".format(np.allclose(\n",
    "    your_dw, conv2d_backward_tf(X_tf, w_shape, b_tf, d_top_tf, stride)[0])\n",
    "))\n",
    "print(\"Are your biases' gradients correct? {}\".format(np.allclose(\n",
    "    your_db, conv2d_backward_tf(X_tf, w_shape, b_tf, d_top_tf, stride)[1])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPool Feedforward\n",
    "\n",
    "Implement a NumPy naive max pool feedforward function. We ask you to simply find the maximum value in your pooling window. Also, don't need to worry about the efficiency of your function. Use loops as many as you like.\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Finish the function __max_pool_forward__ in __utils/layer_funcs.py__. After that, run the following cell blocks, which will give the output of your max pool function. Detailed instructions have been given in the comments of __layer_func.py__. __We need to judge your output to give you credits__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5        -0.48976109 -0.47952218 -0.46928328 -0.45904437 -0.44880546\n",
      "  -0.43856655]\n",
      " [-0.42832765 -0.41808874 -0.40784983 -0.39761092 -0.38737201 -0.37713311\n",
      "  -0.3668942 ]\n",
      " [-0.35665529 -0.34641638 -0.33617747 -0.32593857 -0.31569966 -0.30546075\n",
      "  -0.29522184]\n",
      " [-0.28498294 -0.27474403 -0.26450512 -0.25426621 -0.2440273  -0.2337884\n",
      "  -0.22354949]\n",
      " [-0.21331058 -0.20307167 -0.19283276 -0.18259386 -0.17235495 -0.16211604\n",
      "  -0.15187713]\n",
      " [-0.14163823 -0.13139932 -0.12116041 -0.1109215  -0.10068259 -0.09044369\n",
      "  -0.08020478]\n",
      " [-0.06996587 -0.05972696 -0.04948805 -0.03924915 -0.02901024 -0.01877133\n",
      "  -0.00853242]]\n",
      "[[-0.41808874 -0.38737201]\n",
      " [-0.20307167 -0.17235495]]\n",
      "Your feedforward result size :(2, 2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "from utils.layer_funcs import max_pool_forward\n",
    "\n",
    "# Set test parameters.\n",
    "x_shape = (2, 7, 7, 3) #(batch, height, width, channels)\n",
    "x = np.linspace(-0.5, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "print(x[0,:,:,0])\n",
    "\n",
    "pool_size = 2\n",
    "stride = 3\n",
    "\n",
    "your_feedforward = max_pool_forward(x, pool_size, stride)\n",
    "\n",
    "print(your_feedforward[0,:,:,0])\n",
    "print(\"Your feedforward result size :{}\".format(your_feedforward.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.41808874 -0.38737201]\n",
      " [-0.20307167 -0.17235495]], shape=(2, 2), dtype=float64)\n",
      "Is your feedforward correct? True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Verification/checking code. Don't change it.     #\n",
    "####################################################\n",
    "X_tf = tf.constant(x, shape=x_shape)\n",
    "\n",
    "def maxpool_forward_2(x, pool_size, stride):\n",
    "    maxpool_forward = tf.nn.max_pool(x, [1, pool_size, pool_size, 1], [1, stride, stride, 1], padding='VALID')\n",
    "    return maxpool_forward\n",
    "\n",
    "## Print validation result\n",
    "print(maxpool_forward_2(X_tf, pool_size, stride)[0,:,:,0])\n",
    "print(\"Is your feedforward correct? {}\".format(np.allclose(your_feedforward, maxpool_forward_2(X_tf, pool_size, stride))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pool backpropagation\n",
    "\n",
    "Implement a Numpy naive max pooling backpropagation function, referring to the conv2d backpropagation demo. Again, don't worry about the efficiency.\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Finish the function __max_pool_backward__ in __utils/layer_funcs.py__. After that, run the following cell blocks, which will give the output of your backpropagation. Detailed instructions have been given in the comments of __layer_func.py__. __We need to judge your output to give you credits__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your inputs' gradients result (size :(2, 7, 7, 3)) is: \n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -1.0856306   0.          0.         -1.50629471  0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -2.42667924  0.          0.         -0.8667404   0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from utils.layer_funcs import max_pool_backward\n",
    "\n",
    "# Set test parameters. Please don't change it.\n",
    "np.random.seed(123)\n",
    "dout = np.random.normal(size=your_feedforward.shape)\n",
    "dx = max_pool_backward(dout, x, pool_size, stride)\n",
    "\n",
    "print(\"Your inputs' gradients result (size :{}) is: \".format(dx.shape))\n",
    "print(dx[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -1.0856306   0.          0.         -1.50629471  0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -2.42667924  0.          0.         -0.8667404   0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]], shape=(7, 7), dtype=float64)\n",
      "Is your inputs' gradients correct? True\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Checking code. Don't change it.     #\n",
    "#######################################\n",
    "d_out_tf = tf.constant(dout, shape=your_feedforward.shape)\n",
    "\n",
    "def max_pool_backward_2(x, d):\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(x)\n",
    "        y = maxpool_forward_2(X_tf, pool_size, stride) * d\n",
    "    dy_dx = g.gradient(y, x)\n",
    "    return dy_dx\n",
    "# ## Print validation result\n",
    "print(max_pool_backward_2(X_tf, d_out_tf)[0,:,:,0])\n",
    "print(\"Is your inputs' gradients correct? {}\".format(np.allclose(dx, max_pool_backward_2(X_tf, d_out_tf))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: TensorFlow CNN (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will construct the CNN in TensorFlow. We will implement a CNN similar to the LeNet structure.\n",
    "\n",
    "TensorFlow offers many useful resources and functions which help developers build the net in a high-level fashion, such as functions in the `layer` module. By utilizing functions in `tf.keras` that exist for Neural Network structuring and training, we can build our own layers and network modules rather quickly.\n",
    "\n",
    "Also, we will introduce a visualization tool called Tensorboard. You can use Tensorboard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data that passes through it.\n",
    "\n",
    "Resources and References: <br>\n",
    "* [TensorBoard: Visualizing Learning](https://www.tensorflow.org/get_started/summaries_and_tensorboard)<br>\n",
    "* [Convolutional Neural Networks (LeNet) - DeepLearning 0.1 documentation](http://deeplearning.net/tutorial/lenet.html)<br>\n",
    "* [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick guide for Tensorboard\n",
    "\n",
    "Tensorboard is a powerful tool provided by TensorFlow. It allows developers to check their graph and trend of parameters. This guide will give you a basic understanding on how to initiate the Tensorboard Jupyter Notebook extension and how to understand the results of the training of your model.\n",
    "\n",
    "For more information, check the official guide on Tensorflow web site [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n",
    "\n",
    "### How to start Tensorboard\n",
    "\n",
    "The cell at the bottom of the Jupyter Notebook should be executed once the model has been trained. In the TensorBoard notebook extension, you will be able to see the training/validation accuracies and loss graphs associated with each model fit. The most recent results can be filtered in the bottom-left hand corner by selecting the most recent training and validation results at the bottom of the list.\n",
    "\n",
    "### Check the graph and summary in Tensorboard\n",
    "\n",
    "After executing the cell once, you should able to see the metrics displayed in the tensorboard. \n",
    "\n",
    "![Tensorboard_2](./utils/notebook_images/Task3_2_2_metrics.png)\n",
    "\n",
    "\n",
    "Also, you may be able to zoom in or zoom out or click into the layer block to check all the variables and tensor operations in the graph, check the trend of the variables and the distribution of those in Scalar, Distributions and Histograms. You may explore the tensorboard by yourself and take advantage to it for debugging the network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> You will try to achieve your own CNN model below that has similar structure to LeNet, show the model graph in tensorboard, and get a model with __90% or higher accuracy__ using the data we provide you. You will use the Keras API to build your model.\n",
    "\n",
    "There is example code for a simplified LeNet model in __utils/neuralnets/cnn/model_LeNet.py__. This sample is used as a guide line for how to build a Neural Net model in Tensorflow using Keras functions. Feel free to study the code and use it to build your own CNN below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span>\n",
    "1. Edit the TODO cell below for the **create_model()** function. Create your own CNN that is based on the LeNet structure to achieve at least **90% test accuracy.**\n",
    "2. Print out the training process and the best validation accuracy, save the model in __model/__ folder.\n",
    "3. Attach a screenshot of your tensorboard graph in the markdown cell below. Double click the cell and replace the example image with your own image. Here is a [Markdown Cheetsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#images) that may also help.\n",
    "\n",
    "__Hint__: You may add/modify layers to your CNN to achieve 90% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from the Fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span>\n",
    "Modify the __create_model()__ function to return a model that can achieve **90% or higher validation accuracy**. For more information on the Keras API, please see https://www.tensorflow.org/api_docs/python/tf/keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with advanced optimizer and learning rate schedule\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 4s 14ms/step - loss: 1.2263 - accuracy: 0.6065 - val_loss: 2.5104 - val_accuracy: 0.1894\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.5645 - accuracy: 0.7961 - val_loss: 0.9017 - val_accuracy: 0.6669\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.4683 - accuracy: 0.8303 - val_loss: 0.4337 - val_accuracy: 0.8241\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.4334 - accuracy: 0.8421 - val_loss: 0.4040 - val_accuracy: 0.8385\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.4025 - accuracy: 0.8521 - val_loss: 0.3509 - val_accuracy: 0.8702\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.3767 - accuracy: 0.8623 - val_loss: 0.3605 - val_accuracy: 0.8664\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.3625 - accuracy: 0.8671 - val_loss: 0.3186 - val_accuracy: 0.8839\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.3477 - accuracy: 0.8722 - val_loss: 0.3159 - val_accuracy: 0.8824\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.3306 - accuracy: 0.8780 - val_loss: 0.3123 - val_accuracy: 0.8822\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.3243 - accuracy: 0.8825 - val_loss: 0.3009 - val_accuracy: 0.8909\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.3178 - accuracy: 0.8837 - val_loss: 0.2921 - val_accuracy: 0.8943\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.3122 - accuracy: 0.8845 - val_loss: 0.3545 - val_accuracy: 0.8634\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.3037 - accuracy: 0.8865 - val_loss: 0.2914 - val_accuracy: 0.8934\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2985 - accuracy: 0.8903 - val_loss: 0.3068 - val_accuracy: 0.8827\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2884 - accuracy: 0.8932 - val_loss: 0.2994 - val_accuracy: 0.8873\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2836 - accuracy: 0.8947 - val_loss: 0.3069 - val_accuracy: 0.8816\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2794 - accuracy: 0.8970 - val_loss: 0.2927 - val_accuracy: 0.8906\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2795 - accuracy: 0.8963 - val_loss: 0.2715 - val_accuracy: 0.8997\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2775 - accuracy: 0.8990 - val_loss: 0.3027 - val_accuracy: 0.8877\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2723 - accuracy: 0.8986 - val_loss: 0.3025 - val_accuracy: 0.8897\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2662 - accuracy: 0.9035 - val_loss: 0.2851 - val_accuracy: 0.8949\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2671 - accuracy: 0.9028 - val_loss: 0.2748 - val_accuracy: 0.8993\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2632 - accuracy: 0.9047 - val_loss: 0.2847 - val_accuracy: 0.8922\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2591 - accuracy: 0.9054 - val_loss: 0.2951 - val_accuracy: 0.8897\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2615 - accuracy: 0.9039 - val_loss: 0.2888 - val_accuracy: 0.8897\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2594 - accuracy: 0.9028 - val_loss: 0.2692 - val_accuracy: 0.9016\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2526 - accuracy: 0.9062 - val_loss: 0.2793 - val_accuracy: 0.8977\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2541 - accuracy: 0.9063 - val_loss: 0.2659 - val_accuracy: 0.9040\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2494 - accuracy: 0.9093 - val_loss: 0.2805 - val_accuracy: 0.9013\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2523 - accuracy: 0.9052 - val_loss: 0.3147 - val_accuracy: 0.8820\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2435 - accuracy: 0.9092 - val_loss: 0.2627 - val_accuracy: 0.9043\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2426 - accuracy: 0.9098 - val_loss: 0.2776 - val_accuracy: 0.9011\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2414 - accuracy: 0.9118 - val_loss: 0.2673 - val_accuracy: 0.9044\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2409 - accuracy: 0.9103 - val_loss: 0.2678 - val_accuracy: 0.9028\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2377 - accuracy: 0.9119 - val_loss: 0.2654 - val_accuracy: 0.9047\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2303 - accuracy: 0.9155 - val_loss: 0.3155 - val_accuracy: 0.8834\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2346 - accuracy: 0.9125 - val_loss: 0.2625 - val_accuracy: 0.9091\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2352 - accuracy: 0.9121 - val_loss: 0.2609 - val_accuracy: 0.9066\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2342 - accuracy: 0.9132 - val_loss: 0.3089 - val_accuracy: 0.8890\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2289 - accuracy: 0.9140 - val_loss: 0.2654 - val_accuracy: 0.9055\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2276 - accuracy: 0.9150 - val_loss: 0.2670 - val_accuracy: 0.9045\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2205 - accuracy: 0.9176 - val_loss: 0.2753 - val_accuracy: 0.8998\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2238 - accuracy: 0.9169 - val_loss: 0.2709 - val_accuracy: 0.9057\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2240 - accuracy: 0.9182 - val_loss: 0.2600 - val_accuracy: 0.9068\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2260 - accuracy: 0.9160 - val_loss: 0.2782 - val_accuracy: 0.9020\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2227 - accuracy: 0.9152 - val_loss: 0.2909 - val_accuracy: 0.8942\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2134 - accuracy: 0.9201 - val_loss: 0.2793 - val_accuracy: 0.9044\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2236 - accuracy: 0.9170 - val_loss: 0.2591 - val_accuracy: 0.9081\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2240 - accuracy: 0.9172 - val_loss: 0.2957 - val_accuracy: 0.8928\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.2123 - accuracy: 0.9215 - val_loss: 0.2833 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f49f8130080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the model, compile the model, and fit it\n",
    "model_test = create_model()\n",
    "model_test.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model_test.fit(x=x_train, \n",
    "          y=y_train,\n",
    "          batch_size=256,\n",
    "          epochs=50, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span>\n",
    "Save the best performing model to the **model/** directory using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/task3_model/assets\n"
     ]
    }
   ],
   "source": [
    "model_test.save(filepath=\"./model/task3_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For future reference, a model can be loaded using load_model() on the file path containing your saved model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 130,186\n",
      "Trainable params: 129,226\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"./model/task3_model\")\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span>\n",
    "Generate the TensorBoard notebook extension and attach a screenshot of the train/test accuracy and loss graphs below the Solution cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2976), started 0:03:42 ago. (Use '!kill 2976' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-260c7f5a0d0be557\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-260c7f5a0d0be557\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ECBM E4040 - Assignment 2- Task 5: Kaggle Open-ended Competition\n",
    "\n",
    "Kaggle is a platform for predictive modelling and analytics competitions in which companies and researchers post data and statisticians and data miners compete to produce the best models for predicting and describing the data.\n",
    "\n",
    "If you don't have a Kaggle account, feel free to join at [www.kaggle.com](https://www.kaggle.com). To let the CAs do the grading more conveniently, please __use Lionmail to join Kaggle__ and __use UNI as your username__.\n",
    "\n",
    "The competition is located here: https://www.kaggle.com/t/b70c5e10a0be487f937f2539dc5c3d04\n",
    "\n",
    "You can find detailed description about this in-class competition on the website above. Please read carefully and follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO__:</span>\n",
    "\n",
    "Train a custom model for the bottle dataset classification problem.\n",
    "- You are free to use any methods taught in the class or found by yourself on the Internet (ALWAYS provide reference to the source).\n",
    "- General training methods include dropout, batch normalization, early stopping, L1/L2 regularization.\n",
    "\n",
    "You are given the test set to generate your predictions\n",
    "- The splitting is 70% public + 30% private, but you don't know which ones are public/private.\n",
    "- Students should achieve an accuracy on the public test set of at least 70%. Two points will be deducted for each 1% below 70% accuracy threshold (i.e. 65% accuracy will have 10 points deducted).\n",
    "\n",
    "The accuracy will be shown on the public leaderboard once you submit your prediction .csv file. The private leaderboard will be released after the competition. The final ranking is based on the private leaderboard results, not the public leaderboard.\n",
    "\n",
    "<span style=\"color:red\">**Note**:</span>\n",
    "* Report your results on the Kaggle, for comparison with other students' optimal results (you can do this several times). \n",
    "* Save your best model.\n",
    "\n",
    "<span style=\"color:red\">**Hint**:</span> You can start from what you have implemented in task 4. Students are allowed to use pretrained networks, and utilize transfer learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW Submission Details:\n",
    "\n",
    "There are two components to reporting the results of this task: \n",
    "\n",
    "**(A) Submission (up to 20 submissions each day) of the .csv prediction file through the Kaggle platform**. You should start doing this __VERY early__, so that students can compare their work as they are making progress with model optimization.\n",
    "\n",
    "**(B) Submitting your best CNN model through Github Classroom repo.**\n",
    "\n",
    "**Note** that assignments are submitted through github classroom only. All code for training your kaggle model should be done in this task 5 jupyter notebook, or in a user defined module (.py file) that is imported for use in the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Information: \n",
    "\n",
    "1. Unzip zip files in GCP or acquire administrator permission for other application installation. When you upload your dataset to your vm instances, you may want to unzip your files. However, unzip command is not built in. To use `sudo apt install unzip` or for future applications installation, you need to: \n",
    "  - Login as `ecbm4040` as is exampled [in the tutorial](https://ecbme4040.github.io/2023_fall/EnvSetup/gcp.html#2_Connect_to_your_instance_22)\n",
    "  - Run `sudo apt install unzip`\n",
    "  - You might experience different errors on this step, e.g.\n",
    "    - Permission Denied: Command `apt(-get)` will only run with `sudo` or under `root`. Don't forget the `sudo` prefix or switch to root by `sudo su`.\n",
    "    - Require password: Change Linux username **before** SSH login as is exampled in the tutorial. If you do `su ecbm4040` after SSH login with other users, you will need password to use `sudo`.\n",
    "    - Resource Temporarily Unavailable: Restart the VM or kill the processes occupying the resource. Google for solutions or refer to [this link](https://itsfoss.com/could-not-get-lock-error/) for details.\n",
    "    - Others: Google for solutions or contact TAs.\n",
    "\n",
    "2. If you meet kernel crash (or the running never ends), you might consider using a larger memory CPU.Especially if you include large network structure like VGG, 15GB memory or more CPU is recommended\n",
    "\n",
    "3. Some python libraries that you might need to install first include pandas, scikit-learn. there are **2 OPTIONS** that you can use to install them:\n",
    "  - In the envTF24 environment in linux interface, type: `pip install [PACKAGE]` \n",
    "  - In the jupyter notebook (i.e. this file), type `!pip install [PACKAGE]`. Sometimes you need to restart the virtual environment or even the instance to get these packages functional.\n",
    "\n",
    "4. You might need extra pip libraries to handle dataset, include network, etc. You can follow step 3 to install them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">__Submission:__</span>\n",
    "\n",
    "(i) In your Assignment 2 submission folder, create a subfolder called __KaggleModel__. Save your best model using `model.save()`. This will generate a `saved_model.pb` file, a folder called `variables`, and a folder called `checkpoints` all inside the __KaggleModel__ folder. Only upload your best model. \n",
    "\n",
    "(ii) <span style=\"color:red\">If your saved model exceeds 100 MB, \".gitignore\" it or you will get an error when pushing.</span> **Upload the model to Google Drive and explicitly provide the link under the 'Save your best model' cell.**\n",
    "\n",
    "(iii) Remember to delete any intermediate results, we only want your best model. Do not upload any data files. The instructors will rerun the uploaded best model and verify against the score which you reported on the Kaggle.\n",
    "\n",
    "**The top 10 final submissions of the Kaggle competition will receive up to 10 bonus points proportional to the private test accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two options to load Kaggle data.\n",
    "\n",
    "**Option 1**:\n",
    "1. Manually download the data from kaggle.\n",
    "2. Upload the data to GCP via Jupyter or SSH.\n",
    "3. Unzip the files using command `unzip` or a Python script (you may use the [`zipfile`](https://docs.python.org/3/library/zipfile.html) package).\n",
    "4. Move the dataset to your favorite location for the following tasks. Be careful not to upload them to Github.\n",
    "\n",
    "**Option 2**:\n",
    "1. Login as `ecbm4040`.\n",
    "2. Upload your API key to \"**~/.kaggle/kaggle.json**\" folder following the instructions in https://github.com/Kaggle/kaggle-api#api-credentials.\n",
    "3. Run in console:\n",
    "    ```\n",
    "    chmod 600 ~/.kaggle/kaggle.json\n",
    "    pip install kaggle\n",
    "    kaggle competitions download -c ecbm4040-fall2023\n",
    "    sudo apt install unzip\n",
    "    unzip ecbm4040-fall2023.zip\n",
    "    ```\n",
    "4. Now the data should be under your current directory (\"**/home/ecbm4040/**\" by default).\n",
    "5. Move the dataset to your favorite location for the following tasks. Be careful not to upload them to Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ecbm4040-fall2023.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder 1\n",
      "Reading folder 3\n",
      "Reading folder 0\n",
      "Reading folder 2\n",
      "Reading folder 4\n",
      "Reading Test Images\n",
      "Training data shape:  (15000, 128, 128, 3)\n",
      "Training labels shape:  (15000,)\n",
      "Test data shape:  (3500, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "#Generate dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "root = \"./assignment2-task5/\" #TODO: Enter your path\n",
    "\n",
    "#Load Training images and labels\n",
    "train_directory = root + \"kaggle_train_128/train_128\"\n",
    "image_list=[]\n",
    "label_list=[]\n",
    "for sub_dir in os.listdir(train_directory):\n",
    "    print(\"Reading folder {}\".format(sub_dir))\n",
    "    sub_dir_name=os.path.join(train_directory,sub_dir)\n",
    "    for file in os.listdir(sub_dir_name):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_list.append(np.array(Image.open(os.path.join(sub_dir_name,file))))\n",
    "            label_list.append(int(sub_dir))\n",
    "X_train = np.array(image_list)\n",
    "y_train = np.array(label_list)\n",
    "\n",
    "#Load Test images\n",
    "test_directory = root + \"kaggle_test_128/test_128\"\n",
    "test_image_list=[]\n",
    "test_df = pd.DataFrame([], columns=['Id', 'X'])\n",
    "print(\"Reading Test Images\")\n",
    "for file in os.listdir(test_directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        test_df = test_df.append({\n",
    "            'Id': filename,\n",
    "            'X': np.array(Image.open(os.path.join(test_directory,file)))\n",
    "        }, ignore_index=True)\n",
    "\n",
    "test_df['s'] = [int(x.split('.')[0]) for x in test_df['Id']]\n",
    "test_df = test_df.sort_values(by=['s'])\n",
    "test_df = test_df.drop(columns=['s'])\n",
    "X_test = np.stack(test_df['X'])\n",
    "\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Your Model Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in /home/ecbm4040/envTF24/lib/python3.6/site-packages (0.5.1)\r\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "!pip install split-folders\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import pathlib, splitfolders, os, math\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (15000, 128, 128, 3)\n",
      "Training labels shape:  (15000,)\n",
      "Test data shape:  (3500, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, padding, pool_size, dropout_rate):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.C1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)\n",
    "        self.B1 = BatchNormalization()\n",
    "        self.A1 = Activation('relu')\n",
    "        self.P1 = MaxPooling2D(pool_size=pool_size, strides=2, padding=padding)\n",
    "        self.Dr1 = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.B1(x)\n",
    "        x = self.A1(x)\n",
    "        x = self.P1(x)\n",
    "        y = self.Dr1(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, dropout_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.D1 = Dense(units, activation='relu')\n",
    "        self.B1 = BatchNormalization()\n",
    "        self.D2 = Dense(units * 2, activation='relu')\n",
    "        self.D3 = Dense(units * 2, activation='relu')\n",
    "        self.D4 = Dense(units, activation='relu')\n",
    "        self.Dr1 = Dropout(dropout_rate)\n",
    "        self.D5 = Dense(5, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.D1(x)\n",
    "        x = self.B1(x)\n",
    "        x = self.D2(x)\n",
    "        x = self.D3(x)\n",
    "        x = self.D4(x)\n",
    "        x = self.Dr1(x)\n",
    "        y = self.D5(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.C1 = Conv2D(filters=32, kernel_size=(3 * 3), strides=1, padding='same', input_shape=input_shape)\n",
    "        self.B1 = BatchNormalization()\n",
    "        self.A1 = Activation('relu')\n",
    "        \n",
    "        self.layer1 = CNNBlock(filters=32, kernel_size=(3 * 3), strides=1, padding='same', pool_size=(2 * 2), dropout_rate=0.3)  #filters=output size, kernel size=size of filter, stride=no. of steps, padding=not used, pool=combining outputs\n",
    "        self.layer2 = CNNBlock(filters=64, kernel_size=(3 * 3), strides=1, padding='same', pool_size=(2 * 2), dropout_rate=0.4)\n",
    "        self.layer3 = CNNBlock(filters=32, kernel_size=(3 * 3), strides=1, padding='same', pool_size=(2 * 2), dropout_rate=0.3)\n",
    "        \n",
    "        self.F1 = Flatten()\n",
    "        self.layer4 = DenseBlock(units=64, dropout_rate=0.3)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.B1(x)\n",
    "        x = self.A1(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.F1(x)\n",
    "        \n",
    "        y = self.layer4(x)\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        name = 'Bangle_Net'\n",
    "        return name\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = tf.keras.Input(shape=(300,300,3))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 5\n",
    "y_train_ohe = tf.one_hot(y_train,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 128, 128\n",
    "input_shape = (img_height, img_width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "\n",
    "net.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "checkpoint_save_path = './checkpoint/Baseline.ckpt'\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    net.load_weights(checkpoint_save_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True,\n",
    "                                                 save_best_only=True)\n",
    "\n",
    "history = net.fit(X_train, y_train_ohe, epochs=20, batch_size=32, callbacks=[cp_callback])\n",
    "\n",
    "net.summary()\n",
    "\n",
    "file = open('./weights.txt', 'w')\n",
    "for v in net.trainable_variables:\n",
    "    file.write(str(v.name) + '\\n')\n",
    "    file.write(str(v.shape) + '\\n')\n",
    "    file.write(str(v.numpy()) + '\\n')\n",
    "\n",
    "file.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your best model\n",
    "\n",
    "**Link to large model on Google Drive: [insert link here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "net.save('final', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate .csv file for Kaggle\n",
    "\n",
    "The following code snippet can be used to generate your prediction .csv file.\n",
    "\n",
    "NOTE: If your Kaggle results are indicating random performance, then it's likely that the indices of your csv predictions are misaligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "\n",
    "# Assuming 'root' is defined and points to the root directory\n",
    "root = \"./assignment2-task5/\"  # replace with your root directory path\n",
    "test_directory = root + 'kaggle_test_128/test_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = tf.keras.models.load_model('/content/final')  # replace with your model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_list = []\n",
    "test_df = pd.DataFrame([], columns=['Id', 'X'])\n",
    "print(\"Reading Test Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(test_directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(test_directory, file)\n",
    "        image = np.array(Image.open(image_path).convert('RGB'))  # Ensure images are in RGB\n",
    "        test_df = test_df.append({'Id': filename, 'X': image}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by image ID\n",
    "test_df['s'] = [int(x.split('.')[0]) for x in test_df['Id']]\n",
    "test_df = test_df.sort_values(by=['s'])\n",
    "test_df = test_df.drop(columns=['s'])\n",
    "\n",
    "# Stack the images into a numpy array\n",
    "X_test = np.stack(test_df['X'])\n",
    "\n",
    "# Normalize the images if your model expects values between 0 and 1\n",
    "X_test = X_test.astype('float32') \n",
    "\n",
    "print('Test data shape: ', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions\")\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Id': test_df['Id'],\n",
    "    'label': predicted_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predict_labels.csv', index=False, header=True)\n",
    "print(\"Predictions saved to predict_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Drive Link\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
